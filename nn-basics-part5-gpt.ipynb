{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "20e0c9ac",
      "metadata": {
        "id": "20e0c9ac"
      },
      "source": [
        "## Build the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install convokit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xzQhse7PUFr",
        "outputId": "38cae9f4-cc50-4895-d258-707a691820dc"
      },
      "id": "4xzQhse7PUFr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-3.0.0.tar.gz (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n",
            "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
            "Collecting dill>=0.2.9 (from convokit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n",
            "Collecting clean-text>=0.6.0 (from convokit)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.1)\n",
            "Collecting pymongo>=4.0 (from convokit)\n",
            "  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n",
            "Collecting dnspython>=1.16.0 (from convokit)\n",
            "  Downloading dnspython-2.5.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.4/305.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.6.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy>=2.3.5->convokit) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.3)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=fa80c8f014582753a7f5170a93ad1d1b5153c7830e658c5851c27004a49fa2e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=c906fe7c47d3d910ca7773a945f9a6c5a4434db1310e59156988b4463cdb941a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.7 dnspython-2.5.0 emoji-1.7.0 ftfy-6.1.3 msgpack-numpy-0.4.8 pymongo-4.6.1 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f462b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f462b91",
        "outputId": "0a1e6085-a236-45f8-efc8-43c3ef0f7cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading friends-corpus to /root/.convokit/downloads/friends-corpus\n",
            "Downloading friends-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/friends-corpus/friends-corpus.zip (6.1MB)... Done\n",
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "default_backend: mem\n",
            "Rachel Green\n",
            "Well, can I keep the presents and still be 29?\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from convokit import Corpus, download\n",
        "\n",
        "# filename = \"~/.convokit/downloads/friends-corpus\"\n",
        "# corpus = Corpus(filename=os.path.expanduser(filename))\n",
        "corpus = Corpus(download('friends-corpus'))\n",
        "\n",
        "utterance = corpus.get_utterance('s07_e14_c01_u018')\n",
        "print(utterance.speaker.id)\n",
        "print(utterance.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "95ce4f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95ce4f0d",
        "outputId": "2c436210-4e3f-4c13-cc1c-93149a13ad14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monica Geller\n",
            "There's nothing to tell! He's just some guy I work with!\n",
            "\n",
            "Joey Tribbiani\n",
            "C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
            "\n",
            "Chandler Bing\n",
            "All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
            "\n",
            "Phoebe Buffay\n",
            "Wait, does he eat chalk?\n",
            "\n",
            "Phoebe Buffay\n",
            "Just, 'cause, I don't want her to go through what I went through with Carl oh!\n",
            "\n",
            "Monica Geller\n",
            "Okay, everybody relax. This is not even a date. It's just two people going out to dinner and no\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "re_pattern = \"[^0-9a-zA-Z,.?!' ]\"\n",
        "\n",
        "all_utterance = []\n",
        "\n",
        "for utterance in corpus.iter_utterances():\n",
        "    speaker = utterance.speaker.id\n",
        "    if speaker == \"TRANSCRIPT_NOTE\":\n",
        "        # Only interested in conversations\n",
        "        continue\n",
        "    speaker = re.sub(re_pattern, '', speaker)\n",
        "    text = re.sub(re_pattern, '', utterance.text)\n",
        "    all_utterance.append(f\"{speaker}\\n{text}\")\n",
        "\n",
        "n = int(len(all_utterance) * 0.9)\n",
        "train_data_text = '\\n\\n'.join(all_utterance[:n])\n",
        "val_data_text = '\\n\\n'.join(all_utterance[n:])\n",
        "\n",
        "print(train_data_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5858461d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5858461d",
        "outputId": "3a895c67-27f9-4079-d44c-e8bebe9cd33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary size: 69\n",
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, ',': 4, '.': 5, '0': 6, '1': 7, '2': 8, '3': 9, '4': 10, '5': 11, '6': 12, '7': 13, '8': 14, '9': 15, '?': 16, 'A': 17, 'B': 18, 'C': 19, 'D': 20, 'E': 21, 'F': 22, 'G': 23, 'H': 24, 'I': 25, 'J': 26, 'K': 27, 'L': 28, 'M': 29, 'N': 30, 'O': 31, 'P': 32, 'Q': 33, 'R': 34, 'S': 35, 'T': 36, 'U': 37, 'V': 38, 'W': 39, 'X': 40, 'Y': 41, 'Z': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68}\n"
          ]
        }
      ],
      "source": [
        "all_characters = sorted(list(set(train_data_text)))\n",
        "stoi = {s:i for i, s in enumerate(sorted(all_characters))}\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "\n",
        "print(\"Dictionary size:\", len(stoi))\n",
        "print(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b6d8b7b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6d8b7b9",
        "outputId": "6caa071a-e777-4485-f21a-4ecd5451afd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([24, 47, 54, 54, 57,  1, 65, 57, 60, 54, 46])\n",
            "Hello world\n"
          ]
        }
      ],
      "source": [
        "encode = lambda s: torch.tensor([stoi[c] for c in s])\n",
        "decode = lambda c: ''.join([itos[v.item()] for v in c])\n",
        "\n",
        "train_data = encode(train_data_text)\n",
        "val_data = encode(val_data_text)\n",
        "\n",
        "print(encode(\"Hello world\"))\n",
        "print(decode(encode(\"Hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bf473b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf473b69",
        "outputId": "f2bffae7-118d-4ac6-8183-2d6635975c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "tensor([52, 43, 45, 53, 47, 62,  5,  0], device='cuda:0') -> tensor([43, 45, 53, 47, 62,  5,  0,  0], device='cuda:0')\n",
            "\n",
            "tensor([52], device='cuda:0') -> 43\n",
            "j -> a\n",
            "tensor([52, 43], device='cuda:0') -> 45\n",
            "ja -> c\n",
            "tensor([52, 43, 45], device='cuda:0') -> 53\n",
            "jac -> k\n",
            "tensor([52, 43, 45, 53], device='cuda:0') -> 47\n",
            "jack -> e\n",
            "tensor([52, 43, 45, 53, 47], device='cuda:0') -> 62\n",
            "jacke -> t\n",
            "tensor([52, 43, 45, 53, 47, 62], device='cuda:0') -> 5\n",
            "jacket -> .\n",
            "tensor([52, 43, 45, 53, 47, 62,  5], device='cuda:0') -> 0\n",
            "jacket. -> \n",
            "\n",
            "tensor([52, 43, 45, 53, 47, 62,  5,  0], device='cuda:0') -> 0\n",
            "jacket.\n",
            " -> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(100)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        data = train_data\n",
        "    elif split == 'val':\n",
        "        data = val_data\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return (x, y)\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(x[0], '->', y[0])\n",
        "print()\n",
        "for t in range(block_size):\n",
        "    print(f\"{x[0, :t+1]} -> {y[0, t]}\")\n",
        "    print(f\"{decode(x[0, :t+1])} -> {decode(y[0, t].view(-1))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6fe530",
      "metadata": {
        "id": "2a6fe530"
      },
      "source": [
        "# A simple MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "217898a1",
      "metadata": {
        "id": "217898a1"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_emb, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_emb, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_emb),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a7fb60c4",
      "metadata": {
        "id": "a7fb60c4"
      },
      "outputs": [],
      "source": [
        "d_vocab = len(stoi)\n",
        "d_emb = 64\n",
        "d_ff = 128\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "# v0.1\n",
        "class SimpleLanguageModel(nn.Module):\n",
        "    '''Simple Bigram language model with a single feedforward layer'''\n",
        "    def __init__(self, d_vocab, d_emb, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(d_vocab, d_emb)  # B, T -> B, T, d_emb\n",
        "        self.ffwd = FeedForward(d_emb, d_ff, dropout)  # B, T, d_emb -> B, T, d_emb\n",
        "        self.lm_head = nn.Linear(d_emb, d_vocab)  # B, T, d_emb -> B, T, d_vocab\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # B, T = x.shape\n",
        "        x = self.emb(x) # B, T, d_emb\n",
        "        logits = self.lm_head(x) # B, T, d_vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, d_vocab = logits.shape\n",
        "            logits = logits.view(-1, d_vocab)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, n_tokens):\n",
        "        # B, T = x.shape\n",
        "        for _ in range(n_tokens):\n",
        "            x_cond = x[:, -block_size:]\n",
        "            logits, loss = self(x_cond)  # logits: B, T, d_vocab\n",
        "            logits = logits[:, -1, :]  # B, d_vocab\n",
        "            probs = F.softmax(logits, dim=-1)  # B, d_vocab\n",
        "            x_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1)  # B, T+1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9ff65c58",
      "metadata": {
        "id": "9ff65c58"
      },
      "outputs": [],
      "source": [
        "model = SimpleLanguageModel(d_vocab, d_emb, d_ff, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d6d07a8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6d07a8c",
        "outputId": "cf67778d-b19b-472d-ce4e-b5efd3209b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 69])\n",
            "tensor(4.2356, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train')\n",
        "xb, yb = xb[:1], yb[:1]\n",
        "logit, loss = model(xb, yb)\n",
        "print(logit.shape)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "838b7d29",
      "metadata": {
        "id": "838b7d29"
      },
      "outputs": [],
      "source": [
        "# v0.2 - with the generate function\n",
        "class SimpleLanguageModel(nn.Module):\n",
        "    '''Simple Bigram language model with a single feedforward layer'''\n",
        "    def __init__(self, d_vocab, d_emb, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(d_vocab, d_emb)  # B, T -> B, T, d_emb\n",
        "        self.ffwd = FeedForward(d_emb, d_ff, dropout)  # B, T, d_emb -> B, T, d_emb\n",
        "        self.lm_head = nn.Linear(d_emb, d_vocab)  # B, T, d_emb -> B, T, d_vocab\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # B, T = x.shape\n",
        "        x = self.emb(x) # B, T, d_emb\n",
        "        logits = self.lm_head(x) # B, T, d_vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, d_vocab = logits.shape\n",
        "            logits = logits.view(-1, d_vocab)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, n_tokens):\n",
        "        # B, T = x.shape\n",
        "        for _ in range(n_tokens):\n",
        "            x_cond = x[:, -block_size:]\n",
        "            logits, loss = self(x_cond)  # logits: B, T, d_vocab\n",
        "            logits = logits[:, -1, :]  # B, d_vocab\n",
        "            probs = F.softmax(logits, dim=-1)  # B, d_vocab\n",
        "            x_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1)  # B, T+1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 183,
      "id": "17c32424",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17c32424",
        "outputId": "edcce34f-5d27-4258-d338-2b0a223d32e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25477\n",
            "tensor([[ 0,  9, 28,  1, 11,  1, 38, 40, 35, 11, 21, 63, 22, 31, 67,  9,  2, 19,\n",
            "         26, 28, 37, 32, 45, 48, 50,  8, 51, 35, 52, 27, 58, 50, 31, 21, 15, 55,\n",
            "          0, 42, 14, 12,  5, 35, 35, 17, 28,  7,  1, 62, 56, 48, 23, 20, 20, 60,\n",
            "          4, 48,  6, 31, 35,  9, 50,  5, 13, 17, 13, 52,  3, 26, 12, 63, 15, 15,\n",
            "         15, 64, 24, 34, 13, 14, 23, 44, 68, 56, 14, 35, 16, 21, 43, 20, 47, 22,\n",
            "         32,  4, 49, 58, 34,  8,  0,  8, 65,  8, 58]], device='cuda:0')\n",
            "\n",
            "3L 5 VXS5EuFOy3!CJLUPcfh2iSjKphOE9m\n",
            "Z86.SSAL1 tnfGDDr,f0OS3h.7A7j'J6u999vHR78Gbzn8S?EaDeFP,gpR2\n",
            "2w2p\n"
          ]
        }
      ],
      "source": [
        "model = SimpleLanguageModel(d_vocab, d_emb, d_ff, dropout).to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "gen_text = model.generate(x=torch.zeros((1, 1), dtype=torch.long).to(device), n_tokens=100)\n",
        "print(gen_text)\n",
        "print(decode(gen_text[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6260435",
      "metadata": {
        "id": "c6260435"
      },
      "source": [
        "# Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "id": "064d9d76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064d9d76",
        "outputId": "d14c3c4f-d521-4eb1-8788-83515c6e6a65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.233867290019989"
            ]
          },
          "metadata": {},
          "execution_count": 179
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters=200):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    for i in range(eval_iters):\n",
        "        x, y = get_batch('val')\n",
        "        logits, loss = model(x, y)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / eval_iters\n",
        "\n",
        "estimate_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "id": "6d575624",
      "metadata": {
        "id": "6d575624"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 181,
      "id": "817fff9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817fff9c",
        "outputId": "fb693560-9496-46d8-be75-88a69c006af9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 4.233941078186035\n",
            "Validation loss: 4.23217838048935\n",
            "5000 2.3934381008148193\n",
            "10000 2.3754165172576904\n",
            "15000 2.2746267318725586\n",
            "20000 2.4064085483551025\n",
            "Validation loss: 2.4029186367988586\n",
            "25000 2.3725385665893555\n",
            "30000 2.3118181228637695\n",
            "35000 2.3217997550964355\n",
            "40000 2.395559787750244\n",
            "Validation loss: 2.3980125439167024\n",
            "45000 2.3658084869384766\n",
            "50000 2.5170881748199463\n",
            "55000 2.466986656188965\n",
            "60000 2.441925525665283\n",
            "Validation loss: 2.394424878358841\n",
            "65000 2.2491915225982666\n",
            "70000 2.4393274784088135\n",
            "75000 2.30452823638916\n",
            "80000 2.3951332569122314\n",
            "Validation loss: 2.399096369743347\n",
            "85000 2.3104934692382812\n",
            "90000 2.4537477493286133\n",
            "95000 2.4453818798065186\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 5000 == 0:\n",
        "        print(steps, loss.item())\n",
        "        if steps % 20000 == 0:\n",
        "            print(\"Validation loss:\", estimate_loss(model))\n",
        "\n",
        "    # comment out after confirmed it works\n",
        "    # if steps >= 5000:\n",
        "    #     break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 184,
      "id": "18868156",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18868156",
        "outputId": "91c5cd24-b5b9-4827-fa12-178fafa45bd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Docherengowand the Cha I'the s ing.\n",
            "\n",
            "\n",
            "\n",
            "I y I'm Geveba Thenke p ang s wid ana Jo I'tou we knoy'sus leeaure Any bifowinge wh ang\n",
            "Moufon matme, Rind yonewa Caksshe Th Thanonagou'ra g m ss chandat w I tr ateasebelit d hicastufand. g...\n",
            "\n",
            "Ohey ouher yo inewosh wss Cho mey su see'sit anyo, wn, s hey Bitooutale'st ff walle\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Try th, s Bufithr feat cy inost'cksenore ly as!\n",
            "Y'ves Han\n",
            "\n",
            "\n",
            "\n",
            "Jol I y choway?\n",
            "Uhtha h. d n rin hthe s Tr Bat.. Bite I m. Fos ene wangh, t k l.\n",
            "Many l her\n",
            "\n",
            "Rotoooust knd Gerenonibbbbbelerelait'vin! s s t e, in 's cayowrryou, alf d e Hine Ge Tr\n",
            "Ravalee'd Bililesot ost Buyo, bethang\n",
            "Seboey, thas l! d nin\n",
            "Row! t I t.\n",
            "\n",
            "Whelllle tehatoer\n",
            "Oheld y?\n",
            "Soiverechellin outiomeaver llaffonngoularts?\n",
            "\n",
            "Wenk. Phi\n",
            "Yabacan.. wouth. meros Ran'knn\n",
            "Joull t 'h, a.\n",
            "Rfor sst n'mebanyoo qu y kan Eheyor hon's s?\n",
            "Eaurethay?! copar\n",
            "Rag waheyore g.\n",
            "Four I'tre owam dom d y pl Ph. t?\n",
            "\n",
            "Troey Bume g\n",
            "\n",
            "\n",
            "Th I'sioealy! tre y mbe wan'leelllorr s. ite llp lerin uf ju Bitho tele?\n",
            "Roo Chatryont Budnghaly?\n",
            "Nibenelle ufelerX'liat doertini\n",
            "A!! t areal Houibe r me Ge Routh.. Geanoe y\n",
            "Rayore f piveysut Gelet oulel dian.\n",
            "Recor\n",
            "\n",
            "Heng arel ss. teakncheveast? yongowranthay.\n",
            "Wereanaghet'cath Jor, inothe I shithetit as me Grevirsthallly?\n",
            "\n",
            "I's Tre itan'mellid pareleyoe acrer\n",
            "Gee dy? o ppoveng? u.\n",
            "\n",
            "Roel I moffowhy u'reacecare knelighellid I ag caks fat Auzabit an\n",
            "\n",
            "\n",
            "I y\n",
            "Ohar s Bich, su cacknt's lyow, tieait t Ge our\n",
            "Oh.\n",
            "Bu Bibindid, wou Tre Teissse favelely'salemaw? s Grcu Ma onnoummiatooe h foe Ok dly'thofassn ele t ou y, atr Sourewe. Genchandllllofas Bigot I p Choingenoent?\n",
            "\n",
            "Jower th ate tedor cu youylkakelliay\n",
            "\n",
            "\n",
            "Ohey achon.\n",
            "Ocamot the Thtouad fon ble'Che, Godin meat.\n",
            "Wit. tam wh, Bi\n",
            "\n",
            "Raacan'me! Bugoee.\n",
            "Jongr Thar\n",
            "Mour coen'm t Jo ndd I hit, ll t Gree you woy at e?\n",
            "Whalet Bith, w ay s I hou.. my Godaye t th, k.\n",
            "\n",
            "Theleared opoucosoh.\n",
            "\n",
            "\n",
            "Cheasso louto'sel, haveche lyohe Na blathe'sonounthat, w tid ht Ge ahacct w?! Gr t! blit..\n",
            "Hermer sellle Mat's s Gr w.\n",
            "Yonn's. Em wey, g s pegoeell jan'storinondng\n",
            "\n",
            "Man can,\n"
          ]
        }
      ],
      "source": [
        "# Generation\n",
        "print(decode(\n",
        "    model.generate(x=torch.zeros((1, 1), dtype=torch.long).to(device), n_tokens=2000)[0]\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842bb0b2",
      "metadata": {
        "id": "842bb0b2"
      },
      "source": [
        "## Self attention\n",
        "\n",
        "Self attention is a communication mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2bf4eab0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf4eab0",
        "outputId": "d8c12d81-b14f-412a-c5fd-3dfd64d8603e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values = \n",
            " tensor([[5., 8.],\n",
            "        [0., 5.],\n",
            "        [3., 4.]])\n",
            "----------\n",
            "first_k_avg = \n",
            " tensor([[5.0000, 8.0000],\n",
            "        [2.5000, 6.5000],\n",
            "        [2.6667, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(415)\n",
        "values = torch.randint(0, 10, (3, 2)).float()\n",
        "\n",
        "# Version 1: naive for-loop\n",
        "first_k_avg = torch.empty_like(values)\n",
        "for i in range(first_k_avg.shape[0]):\n",
        "    for j in range(first_k_avg.shape[1]):\n",
        "        first_k_avg[i, j] = values[:i + 1, j].mean()\n",
        "\n",
        "print(\"values = \\n\", values)\n",
        "print('-' * 10)\n",
        "print(\"first_k_avg = \\n\", first_k_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6de91d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6de91d83",
        "outputId": "7f412788-86fb-4d8c-9589-f82e05d08093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei = \n",
            " tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "----------\n",
            "first_k_avg = \n",
            " tensor([[5.0000, 8.0000],\n",
            "        [2.5000, 6.5000],\n",
            "        [2.6667, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "# Version 2: mat mul\n",
        "num_rows = values.shape[0]\n",
        "wei = torch.tril(torch.ones(num_rows, num_rows))\n",
        "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
        "print(\"wei = \\n\", wei)\n",
        "print('-' * 10)\n",
        "print(\"first_k_avg = \\n\", wei @ values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "39ba4a3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ba4a3c",
        "outputId": "88c6e630-b27b-40ea-bc46-9d6f90f4a2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei before softmax:\n",
            " tensor([[0., -inf, -inf],\n",
            "        [0., 0., -inf],\n",
            "        [0., 0., 0.]])\n",
            "----------\n",
            "wei after softmax:\n",
            " tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "----------\n",
            "first_k_avg = \n",
            " tensor([[5.0000, 8.0000],\n",
            "        [2.5000, 6.5000],\n",
            "        [2.6667, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "# Version 3: softmax\n",
        "wei = torch.zeros((num_rows, num_rows))\n",
        "tril = torch.tril(torch.ones_like(wei))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"wei before softmax:\\n\", wei)\n",
        "print('-' * 10)\n",
        "wei = F.softmax(wei, dim=1)\n",
        "print(\"wei after softmax:\\n\", wei)\n",
        "print('-' * 10)\n",
        "print(\"first_k_avg = \\n\", wei @ values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "df8e9b50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8e9b50",
        "outputId": "0f9b260a-cc3a-4e16-adde-95b67cdf12e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei:\n",
            " tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4607, 0.5393, 0.0000, 0.0000],\n",
            "         [0.3991, 0.2089, 0.3920, 0.0000],\n",
            "         [0.2587, 0.2108, 0.2907, 0.2397]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4038, 0.5962, 0.0000, 0.0000],\n",
            "         [0.2823, 0.3892, 0.3285, 0.0000],\n",
            "         [0.2289, 0.2493, 0.2322, 0.2896]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3789, 0.6211, 0.0000, 0.0000],\n",
            "         [0.3268, 0.3199, 0.3533, 0.0000],\n",
            "         [0.1970, 0.2184, 0.2262, 0.3584]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4430, 0.5570, 0.0000, 0.0000],\n",
            "         [0.2956, 0.3503, 0.3541, 0.0000],\n",
            "         [0.2012, 0.2451, 0.2963, 0.2574]]], grad_fn=<SoftmaxBackward0>)\n",
            "----------\n",
            "weight vector for third prediction:\n",
            " tensor([0.3991, 0.2089, 0.3920, 0.0000], grad_fn=<SelectBackward0>)\n",
            "out shape: torch.Size([4, 4, 16])\n"
          ]
        }
      ],
      "source": [
        "# Version 4: serlf attention\n",
        "\n",
        "B, T, C = 4, 4, 32  # batch, time, channels (embeddings)\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Single head self attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "print(\"wei:\\n\", wei)\n",
        "print('-' * 10)\n",
        "print(\"weight vector for third prediction:\\n\", wei[0, 2])\n",
        "print(\"out shape:\", out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992aa172",
      "metadata": {
        "id": "992aa172"
      },
      "source": [
        "## Notes on Key, Query, Value in self attention\n",
        "\n",
        "Attention is a communication mechanism. It works as a directed graph, passing information along some direction. In text generation, it often involves passing information from past tokens to future tokens.\n",
        "\n",
        "There are three components in self attention mechanism.\n",
        "* Query - Q(x) projects what information x is seeking\n",
        "* Key - K(x) projects what information x contains\n",
        "* Value - V(x) determines what information should be aggregated for the purpose of this single attention head\n",
        "\n",
        "To understand Q, K, V intuitively,\n",
        "* Think of x like private information or private key of a token, it is then projected into the Query, Key, and Value handled by the attention head.\n",
        "* The output of self attention is a weighted sum of the projection V(x), not the tokens themselves. Why?\n",
        "    * It enables us to simultaneously consider various aspects of tokens in different heads after we introduce multi-head attention mechanism next.\n",
        "    * For example, in processing the word \"cat\" within a sentence, different attention heads might aggregate information with regards to its grammatical role (noun), its conceptual meaning as an animal, or its syntactic function as a subject or object. This diversity allows for a richer, more nuanced understanding of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "3a46805e",
      "metadata": {
        "id": "3a46805e"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    '''Single head self attention'''\n",
        "\n",
        "    def __init__(self, d_emb, d_head):\n",
        "        super().__init__()\n",
        "        self.d_head = d_head\n",
        "        self.key = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.query = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.value = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, T, d_emb\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # B, T, dh\n",
        "        q = self.query(x)  # B, T, dh\n",
        "        v = self.value(x)  # B, T, dh\n",
        "        wei = q @ k.transpose(-2, -1) * (self.d_head ** -0.5)  # B, T, T\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v  # B, T, dh\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    '''Multi head self attention'''\n",
        "\n",
        "    def __init__(self, num_heads, d_emb, d_head):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(d_emb, d_head) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * d_head, d_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, T, d_emb\n",
        "        head_out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(head_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55af815e",
      "metadata": {
        "id": "55af815e"
      },
      "source": [
        "## Final Notes on Attention Mechanism\n",
        "\n",
        "1. **Position Encoding**: Attention mechanisms inherently lack a notion of order, unlike convolutions. Therefore, inputs to the attention mechanism should include positional information to maintain sequence context.\n",
        "2. **Scaled Attention**: To prevent the softmax function from collapsing into a one-hot vector, it's crucial that the weights (Q @ K) are diffused appropriately, hence the need for scaling attention (divided by d_head**0.5).\n",
        "3. **Batch Isolation**: Examples within the same batch do not interact; each instance is processed independently.\n",
        "4. **Transformer Architecture Variations**:\n",
        "    - **Decoder Block**: Restricts information flow to prevent future tokens from influencing the output, typically used in output generation phases.\n",
        "    - **Encoder Block**: Allows free communication among all nodes, fully utilizing context, typically used in input interpretation phases.\n",
        "    - **Application in the Transformer paper**: In context of machine translation, the original paper uses Encoder Blocks for source language text, encoding full contextual understanding, and uses Decoder Blocks for target language text, ensuring generated content is influenced only by preceding text and the source content.\n",
        "\n",
        "In addition to self-attention, a transformer block comprises:\n",
        "\n",
        "- **Computation Layer**: a feedforward network computes over the aggregated information, but on per token basis (no communication between two tokens at this step)\n",
        "- **Optimization Techniques for Deep Networks**:\n",
        "    - **Residual Connection**: Facilitates learning by creating shortcuts for gradients, acting as a \"super-highway\" for backpropagation.\n",
        "    - **Layer Normalization**: Standardizes the inputs to each layer, ensuring consistent scale and aiding in stable training.\n",
        "    - **Dropout**: Randomly omits a subset of features at each layer to prevent overfitting and encourage generalized representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f02b6dc",
      "metadata": {
        "id": "7f02b6dc"
      },
      "source": [
        "## Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "1e7183f2",
      "metadata": {
        "id": "1e7183f2"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d(nn.Module):\n",
        "    '''Layer normalization over the last dimension'''\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim, device=device)\n",
        "        self.beta = torch.zeros(dim, device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In batch norm, we aggregate over columns\n",
        "        # In layer norm, we aggregate over rows\n",
        "        xmean = x.mean(-1, keepdim=True)\n",
        "        xvar = x.var(-1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        out = self.gamma * xhat + self.beta\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "c534accc",
      "metadata": {
        "id": "c534accc"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    '''Transformer block with multi-head attention and feedforward'''\n",
        "\n",
        "    def __init__(self, n_emb, n_head):\n",
        "        super().__init__()\n",
        "        d_head = n_emb // n_head\n",
        "        self.attn = MultiHead(n_head, n_emb, d_head)\n",
        "        self.ffwd = FeedForward(n_emb, n_emb, dropout=0.1)\n",
        "        self.ln1 = LayerNorm1d(n_emb)\n",
        "        self.ln2 = LayerNorm1d(n_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, T, n_emb\n",
        "        # with residual connection\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "id": "be931d4b",
      "metadata": {
        "id": "be931d4b"
      },
      "outputs": [],
      "source": [
        "# v0.3 - with the transformer block\n",
        "class SimpleLanguageModelWithTransformer(nn.Module):\n",
        "    '''Simple Bigram language model with transformer layers'''\n",
        "    def __init__(self, d_vocab, d_emb, num_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(d_vocab, d_emb)  # B, T -> B, T, d_emb\n",
        "        self.position_emb = nn.Embedding(block_size, d_emb)  # B, T -> B, T, d_emb\n",
        "        # Note: Changed here\n",
        "        # self.ffwd = FeedForward(d_emb, d_ff, dropout)  # B, T, d_emb -> B, T, d_emb\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[TransformerBlock(d_emb, num_heads) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.ln_final = LayerNorm1d(d_emb)\n",
        "        self.lm_head = nn.Linear(d_emb, d_vocab)  # B, T, d_emb -> B, T, d_vocab\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        _, T = x.shape\n",
        "        token_emb = self.token_emb(x) # B, T, d_emb\n",
        "        pos_emb = self.position_emb(torch.arange(T, device=device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.lm_head(x) # B, T, d_vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, d_vocab = logits.shape\n",
        "            logits = logits.view(-1, d_vocab)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, n_tokens):\n",
        "        # B, T = x.shape\n",
        "        for _ in range(n_tokens):\n",
        "            x_cond = x[:, -block_size:]\n",
        "            logits, loss = self(x_cond)  # logits: B, T, d_vocab\n",
        "            logits = logits[:, -1, :]  # B, d_vocab\n",
        "            probs = F.softmax(logits, dim=-1)  # B, d_vocab\n",
        "            x_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1)  # B, T+1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 174,
      "id": "ca14a08d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca14a08d",
        "outputId": "e6271cbd-926d-4340-af73-a4b984d4e3f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58949\n"
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "num_heads = 4\n",
        "model = SimpleLanguageModelWithTransformer(d_vocab, d_emb, num_heads, n_layer)\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "id": "ccea345f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccea345f",
        "outputId": "bf19db9d-7b73-462a-8cc5-7e7c17ae009a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 4.246626377105713\n",
            "Validation loss: 4.116809515953064\n",
            "5000 1.7877894639968872\n",
            "10000 1.5437992811203003\n",
            "15000 1.7735244035720825\n",
            "20000 1.457893967628479\n",
            "Validation loss: 1.5588035702705383\n",
            "25000 1.652052879333496\n",
            "30000 1.3510609865188599\n",
            "35000 1.6204665899276733\n",
            "40000 1.3851908445358276\n",
            "Validation loss: 1.5322303700447082\n",
            "45000 1.4144151210784912\n",
            "50000 1.4845095872879028\n",
            "55000 1.551680088043213\n",
            "60000 1.5847973823547363\n",
            "Validation loss: 1.5175394719839097\n",
            "65000 1.1751842498779297\n",
            "70000 1.6589553356170654\n",
            "75000 1.348968744277954\n",
            "80000 1.4910104274749756\n",
            "Validation loss: 1.517003682255745\n",
            "85000 1.4565149545669556\n",
            "90000 1.7095304727554321\n",
            "95000 1.4697622060775757\n",
            "Validation loss: 1.4878948092460633\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(100000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 5000 == 0:\n",
        "        print(steps, loss.item())\n",
        "        if steps % 20000 == 0:\n",
        "            print(\"Validation loss:\", estimate_loss(model))\n",
        "\n",
        "    # comment out after confirmed it works\n",
        "    # if steps >= 5000:\n",
        "    #     break\n",
        "print(\"Validation loss:\", estimate_loss(model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation\n",
        "print(decode(\n",
        "    model.generate(x=torch.zeros((1, 1), dtype=torch.long, device=device), n_tokens=2000)[0][8:]\n",
        "))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UmBDz96qi5C",
        "outputId": "c2ba3dc9-b8c6-4c66-bf55-ff7cd9996b34"
      },
      "id": "3UmBDz96qi5C",
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Geller\n",
            "It want so listen! What are you talks.\n",
            "\n",
            "Chright.\n",
            "\n",
            "Rachel Green\n",
            "Hey.\n",
            "\n",
            "Ross Geller\n",
            "Talk I doub uhy chel, like with Mart! I'm gonna go on the ladid!\n",
            "\n",
            "Rachel Green\n",
            "Come look that our do! Till bus! I cause even go me!! Y'know' the come won, some open by I'm the buass that here, I have again Chandler God, tereson. Look. I'm ask the self! It'll exep!\n",
            "\n",
            "Rachel Green\n",
            "Y'know, no think you like I'm somebody want to sance that ask somethrooman out even, who keganbody doesn't be for Chandler Bing\n",
            "Come.\n",
            "\n",
            "Joey Tribbiani\n",
            "You've beenononononey on a wonderonce and a uh!\n",
            "\n",
            "Ross Geller\n",
            "Willior janna peoping bate a look' seem would call alacrazing... you not.\n",
            "\n",
            "Chandler Bing\n",
            "And wlamost because\n",
            "NNo, I have get at that's going? Unto see, don't him than you've pisuck.\n",
            "\n",
            "Chandler!! And uh, when you big out back you can how did it.\n",
            "\n",
            "Rachel Green\n",
            "Well a get me, but is, someone.\n",
            "\n",
            "No you one.\n",
            "\n",
            "Ross Geller\n",
            "Yeah, I toping people find over good! And in to but, you if with you say because bood.\n",
            "\n",
            "Joey Tribbiani\n",
            "So haten.\n",
            "\n",
            "Monica Geller\n",
            "You, they toix\n",
            "Them!\n",
            "\n",
            "Womant. Ben to seeon.\n",
            "\n",
            "Ross Geller\n",
            "I betwould is the mext deced gonna hy in like the him.\n",
            "\n",
            "Monica Geller\n",
            "ALware not so you so xeling the night rolow it's you forgetty time! Hes! Dr. So, you davin' 250 beaugin. Wow!!\n",
            "\n",
            "Phoebe Buffay\n",
            "Oh, I don't think one betten?\n",
            "\n",
            "Phoebe really not there you just have of looking. Buts um, alcour!\n",
            "\n",
            "Joey Tribbiani\n",
            "You need lot out of listing right, mover to can to go hone? Well Jone. Yeah, at was even it.\n",
            "\n",
            "Joey Tribbiani\n",
            "He mabbet it's vestionsons. Poebe Buffay\n",
            "Judying Jone ah Rachel Green\n",
            "Y'know, chick. Monica party oand something? Sorryming to be two about neems. And seet me. No. And into turning you and this, think I have aled like y'know? I mean it! I just her two seap it.\n",
            "\n",
            "Chandler Bing\n",
            "Yeah, me a minute.\n",
            "\n",
            "Rachel Green\n",
            "Ben! Okay1, and Before!\n",
            "\n",
            "Michel Green\n",
            "Oh, you've stumpin innce if and get it's it everyones! Do you have up to menting to these's mom.\n",
            "\n",
            "Monica Geller\n",
            "No don't am over!! Please, here I w\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKjQhPvktlTO"
      },
      "id": "uKjQhPvktlTO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}