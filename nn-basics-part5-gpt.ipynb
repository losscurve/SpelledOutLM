{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "20e0c9ac",
      "metadata": {
        "id": "20e0c9ac"
      },
      "source": [
        "## Build the dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install convokit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xzQhse7PUFr",
        "outputId": "38cae9f4-cc50-4895-d258-707a691820dc"
      },
      "id": "4xzQhse7PUFr",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting convokit\n",
            "  Downloading convokit-3.0.0.tar.gz (183 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.2/183.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.7.1)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.5.3)\n",
            "Collecting msgpack-numpy>=0.4.3.2 (from convokit)\n",
            "  Downloading msgpack_numpy-0.4.8-py2.py3-none-any.whl (6.9 kB)\n",
            "Requirement already satisfied: spacy>=2.3.5 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.6.1)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.11.4)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.2.2)\n",
            "Requirement already satisfied: nltk>=3.4 in /usr/local/lib/python3.10/dist-packages (from convokit) (3.8.1)\n",
            "Collecting dill>=0.2.9 (from convokit)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from convokit) (1.3.2)\n",
            "Collecting clean-text>=0.6.0 (from convokit)\n",
            "  Downloading clean_text-0.6.0-py3-none-any.whl (11 kB)\n",
            "Collecting unidecode>=1.1.1 (from convokit)\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from convokit) (4.66.1)\n",
            "Collecting pymongo>=4.0 (from convokit)\n",
            "  Downloading pymongo-4.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (677 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m677.1/677.1 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.10/dist-packages (from convokit) (6.0.1)\n",
            "Collecting dnspython>=1.16.0 (from convokit)\n",
            "  Downloading dnspython-2.5.0-py3-none-any.whl (305 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m305.4/305.4 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting emoji<2.0.0,>=1.0.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading emoji-1.7.0.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy<7.0,>=6.0 (from clean-text>=0.6.0->convokit)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.0.0->convokit) (2.8.2)\n",
            "Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.10/dist-packages (from msgpack-numpy>=0.4.3.2->convokit) (1.0.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (8.1.7)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk>=3.4->convokit) (2023.6.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.23.4->convokit) (2023.3.post1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->convokit) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.0.10)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (0.11.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (6.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (1.10.13)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (67.7.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy>=2.3.5->convokit) (3.3.0)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy<7.0,>=6.0->clean-text>=0.6.0->convokit) (0.2.13)\n",
            "Requirement already satisfied: pathlib-abc==0.1.1 in /usr/local/lib/python3.10/dist-packages (from pathy>=0.10.0->spacy>=2.3.5->convokit) (0.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy>=2.3.5->convokit) (4.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->convokit) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.3.5->convokit) (2023.11.17)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy>=2.3.5->convokit) (0.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy>=2.3.5->convokit) (2.1.3)\n",
            "Building wheels for collected packages: convokit, emoji\n",
            "  Building wheel for convokit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for convokit: filename=convokit-3.0.0-py3-none-any.whl size=216707 sha256=fa80c8f014582753a7f5170a93ad1d1b5153c7830e658c5851c27004a49fa2e4\n",
            "  Stored in directory: /root/.cache/pip/wheels/c4/89/8c/2677fdb888588b6f93cb6ac86bdfb020f1f1c33e0d5525b231\n",
            "  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoji: filename=emoji-1.7.0-py3-none-any.whl size=171033 sha256=c906fe7c47d3d910ca7773a945f9a6c5a4434db1310e59156988b4463cdb941a\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/8a/8c/315c9e5d7773f74b33d5ed33f075b49c6eaeb7cedbb86e2cf8\n",
            "Successfully built convokit emoji\n",
            "Installing collected packages: emoji, unidecode, msgpack-numpy, ftfy, dnspython, dill, pymongo, clean-text, convokit\n",
            "Successfully installed clean-text-0.6.0 convokit-3.0.0 dill-0.3.7 dnspython-2.5.0 emoji-1.7.0 ftfy-6.1.3 msgpack-numpy-0.4.8 pymongo-4.6.1 unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "4f462b91",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f462b91",
        "outputId": "0a1e6085-a236-45f8-efc8-43c3ef0f7cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading friends-corpus to /root/.convokit/downloads/friends-corpus\n",
            "Downloading friends-corpus from http://zissou.infosci.cornell.edu/convokit/datasets/friends-corpus/friends-corpus.zip (6.1MB)... Done\n",
            "No configuration file found at /root/.convokit/config.yml; writing with contents: \n",
            "# Default Backend Parameters\n",
            "db_host: localhost:27017\n",
            "data_directory: ~/.convokit/saved-corpora\n",
            "default_backend: mem\n",
            "Rachel Green\n",
            "Well, can I keep the presents and still be 29?\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from convokit import Corpus, download\n",
        "\n",
        "# filename = \"~/.convokit/downloads/friends-corpus\"\n",
        "# corpus = Corpus(filename=os.path.expanduser(filename))\n",
        "corpus = Corpus(download('friends-corpus'))\n",
        "\n",
        "utterance = corpus.get_utterance('s07_e14_c01_u018')\n",
        "print(utterance.speaker.id)\n",
        "print(utterance.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "95ce4f0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95ce4f0d",
        "outputId": "2c436210-4e3f-4c13-cc1c-93149a13ad14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Monica Geller\n",
            "There's nothing to tell! He's just some guy I work with!\n",
            "\n",
            "Joey Tribbiani\n",
            "C'mon, you're going out with the guy! There's gotta be something wrong with him!\n",
            "\n",
            "Chandler Bing\n",
            "All right Joey, be nice. So does he have a hump? A hump and a hairpiece?\n",
            "\n",
            "Phoebe Buffay\n",
            "Wait, does he eat chalk?\n",
            "\n",
            "Phoebe Buffay\n",
            "Just, 'cause, I don't want her to go through what I went through with Carl oh!\n",
            "\n",
            "Monica Geller\n",
            "Okay, everybody relax. This is not even a date. It's just two people going out to dinner and no\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "re_pattern = \"[^0-9a-zA-Z,.?!' ]\"\n",
        "\n",
        "all_utterance = []\n",
        "\n",
        "for utterance in corpus.iter_utterances():\n",
        "    speaker = utterance.speaker.id\n",
        "    if speaker == \"TRANSCRIPT_NOTE\":\n",
        "        # Only interested in conversations\n",
        "        continue\n",
        "    speaker = re.sub(re_pattern, '', speaker)\n",
        "    text = re.sub(re_pattern, '', utterance.text)\n",
        "    all_utterance.append(f\"{speaker}\\n{text}\")\n",
        "\n",
        "n = int(len(all_utterance) * 0.9)\n",
        "train_data_text = '\\n\\n'.join(all_utterance[:n])\n",
        "val_data_text = '\\n\\n'.join(all_utterance[n:])\n",
        "\n",
        "print(train_data_text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "5858461d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5858461d",
        "outputId": "3a895c67-27f9-4079-d44c-e8bebe9cd33d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dictionary size: 69\n",
            "{'\\n': 0, ' ': 1, '!': 2, \"'\": 3, ',': 4, '.': 5, '0': 6, '1': 7, '2': 8, '3': 9, '4': 10, '5': 11, '6': 12, '7': 13, '8': 14, '9': 15, '?': 16, 'A': 17, 'B': 18, 'C': 19, 'D': 20, 'E': 21, 'F': 22, 'G': 23, 'H': 24, 'I': 25, 'J': 26, 'K': 27, 'L': 28, 'M': 29, 'N': 30, 'O': 31, 'P': 32, 'Q': 33, 'R': 34, 'S': 35, 'T': 36, 'U': 37, 'V': 38, 'W': 39, 'X': 40, 'Y': 41, 'Z': 42, 'a': 43, 'b': 44, 'c': 45, 'd': 46, 'e': 47, 'f': 48, 'g': 49, 'h': 50, 'i': 51, 'j': 52, 'k': 53, 'l': 54, 'm': 55, 'n': 56, 'o': 57, 'p': 58, 'q': 59, 'r': 60, 's': 61, 't': 62, 'u': 63, 'v': 64, 'w': 65, 'x': 66, 'y': 67, 'z': 68}\n"
          ]
        }
      ],
      "source": [
        "all_characters = sorted(list(set(train_data_text)))\n",
        "stoi = {s:i for i, s in enumerate(sorted(all_characters))}\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "\n",
        "print(\"Dictionary size:\", len(stoi))\n",
        "print(stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b6d8b7b9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6d8b7b9",
        "outputId": "6caa071a-e777-4485-f21a-4ecd5451afd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([24, 47, 54, 54, 57,  1, 65, 57, 60, 54, 46])\n",
            "Hello world\n"
          ]
        }
      ],
      "source": [
        "encode = lambda s: torch.tensor([stoi[c] for c in s])\n",
        "decode = lambda c: ''.join([itos[v.item()] for v in c])\n",
        "\n",
        "train_data = encode(train_data_text)\n",
        "val_data = encode(val_data_text)\n",
        "\n",
        "print(encode(\"Hello world\"))\n",
        "print(decode(encode(\"Hello world\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bf473b69",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bf473b69",
        "outputId": "f2bffae7-118d-4ac6-8183-2d6635975c9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "tensor([52, 43, 45, 53, 47, 62,  5,  0], device='cuda:0') -> tensor([43, 45, 53, 47, 62,  5,  0,  0], device='cuda:0')\n",
            "\n",
            "tensor([52], device='cuda:0') -> 43\n",
            "j -> a\n",
            "tensor([52, 43], device='cuda:0') -> 45\n",
            "ja -> c\n",
            "tensor([52, 43, 45], device='cuda:0') -> 53\n",
            "jac -> k\n",
            "tensor([52, 43, 45, 53], device='cuda:0') -> 47\n",
            "jack -> e\n",
            "tensor([52, 43, 45, 53, 47], device='cuda:0') -> 62\n",
            "jacke -> t\n",
            "tensor([52, 43, 45, 53, 47, 62], device='cuda:0') -> 5\n",
            "jacket -> .\n",
            "tensor([52, 43, 45, 53, 47, 62,  5], device='cuda:0') -> 0\n",
            "jacket. -> \n",
            "\n",
            "tensor([52, 43, 45, 53, 47, 62,  5,  0], device='cuda:0') -> 0\n",
            "jacket.\n",
            " -> \n",
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(100)\n",
        "batch_size = 4\n",
        "block_size = 8\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device:\", device)\n",
        "\n",
        "def get_batch(split):\n",
        "    if split == 'train':\n",
        "        data = train_data\n",
        "    elif split == 'val':\n",
        "        data = val_data\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return (x, y)\n",
        "\n",
        "x, y = get_batch('train')\n",
        "print(x[0], '->', y[0])\n",
        "print()\n",
        "for t in range(block_size):\n",
        "    print(f\"{x[0, :t+1]} -> {y[0, t]}\")\n",
        "    print(f\"{decode(x[0, :t+1])} -> {decode(y[0, t].view(-1))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a6fe530",
      "metadata": {
        "id": "2a6fe530"
      },
      "source": [
        "# A simple MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "217898a1",
      "metadata": {
        "id": "217898a1"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_emb, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(d_emb, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_emb),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.ff(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a7fb60c4",
      "metadata": {
        "id": "a7fb60c4"
      },
      "outputs": [],
      "source": [
        "d_vocab = len(stoi)\n",
        "d_emb = 64\n",
        "d_ff = 128\n",
        "dropout = 0.1\n",
        "\n",
        "\n",
        "# v0.1\n",
        "class SimpleLanguageModel(nn.Module):\n",
        "    '''Simple Bigram language model with a single feedforward layer'''\n",
        "    def __init__(self, d_vocab, d_emb, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(d_vocab, d_emb)  # B, T -> B, T, d_emb\n",
        "        self.ffwd = FeedForward(d_emb, d_ff, dropout)  # B, T, d_emb -> B, T, d_emb\n",
        "        self.lm_head = nn.Linear(d_emb, d_vocab)  # B, T, d_emb -> B, T, d_vocab\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # B, T = x.shape\n",
        "        x = self.emb(x) # B, T, d_emb\n",
        "        logits = self.lm_head(x) # B, T, d_vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, d_vocab = logits.shape\n",
        "            logits = logits.view(-1, d_vocab)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, n_tokens):\n",
        "        # B, T = x.shape\n",
        "        for _ in range(n_tokens):\n",
        "            x_cond = x[:, -block_size:]\n",
        "            logits, loss = self(x_cond)  # logits: B, T, d_vocab\n",
        "            logits = logits[:, -1, :]  # B, d_vocab\n",
        "            probs = F.softmax(logits, dim=-1)  # B, d_vocab\n",
        "            x_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1)  # B, T+1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "9ff65c58",
      "metadata": {
        "id": "9ff65c58"
      },
      "outputs": [],
      "source": [
        "model = SimpleLanguageModel(d_vocab, d_emb, d_ff, dropout).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d6d07a8c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6d07a8c",
        "outputId": "cf67778d-b19b-472d-ce4e-b5efd3209b0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 69])\n",
            "tensor(4.2356, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train')\n",
        "xb, yb = xb[:1], yb[:1]\n",
        "logit, loss = model(xb, yb)\n",
        "print(logit.shape)\n",
        "print(loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "838b7d29",
      "metadata": {
        "id": "838b7d29"
      },
      "outputs": [],
      "source": [
        "# v0.2 - with the generate function\n",
        "class SimpleLanguageModel(nn.Module):\n",
        "    '''Simple Bigram language model with a single feedforward layer'''\n",
        "    def __init__(self, d_vocab, d_emb, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(d_vocab, d_emb)  # B, T -> B, T, d_emb\n",
        "        self.ffwd = FeedForward(d_emb, d_ff, dropout)  # B, T, d_emb -> B, T, d_emb\n",
        "        self.lm_head = nn.Linear(d_emb, d_vocab)  # B, T, d_emb -> B, T, d_vocab\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # B, T = x.shape\n",
        "        x = self.emb(x) # B, T, d_emb\n",
        "        logits = self.lm_head(x) # B, T, d_vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, d_vocab = logits.shape\n",
        "            logits = logits.view(-1, d_vocab)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, n_tokens):\n",
        "        # B, T = x.shape\n",
        "        for _ in range(n_tokens):\n",
        "            x_cond = x[:, -block_size:]\n",
        "            logits, loss = self(x_cond)  # logits: B, T, d_vocab\n",
        "            logits = logits[:, -1, :]  # B, d_vocab\n",
        "            probs = F.softmax(logits, dim=-1)  # B, d_vocab\n",
        "            x_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1)  # B, T+1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "17c32424",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17c32424",
        "outputId": "52c8463b-139e-4528-b341-2d6b35211759"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  5, 40, 54, 65, 57, 57, 25, 31, 24, 47, 68, 62, 32, 12, 58, 13, 46,\n",
            "         49,  8, 11, 32, 14, 46, 43, 42,  6,  9,  0, 17, 48, 20,  6, 50, 20, 39,\n",
            "         47, 31, 44,  1,  8, 61, 50, 54, 57, 26, 53, 55, 36, 53, 65, 16, 51, 29,\n",
            "         68,  0, 54, 23, 55, 16,  7,  4, 42,  1, 44, 15, 34, 22, 15, 34, 61, 12,\n",
            "         22, 10, 59, 21, 45,  8, 53, 11, 35,  2, 15,  4, 48, 13, 64, 49, 45, 32,\n",
            "         47, 52, 62, 65, 59, 43, 62, 25, 40,  5, 63]], device='cuda:0')\n",
            "\n",
            ".XlwooIOHeztP6p7dg25P8daZ03\n",
            "AfD0hDWeOb 2shloJkmTkw?iMz\n",
            "lGm?1,Z b9RF9Rs6F4qEc2k5S!9,f7vgcPejtwqatIX.u\n"
          ]
        }
      ],
      "source": [
        "model = SimpleLanguageModel(d_vocab, d_emb, d_ff, dropout).to(device)\n",
        "gen_text = model.generate(x=torch.zeros((1, 1), dtype=torch.long).to(device), n_tokens=100)\n",
        "print(gen_text)\n",
        "print(decode(gen_text[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6260435",
      "metadata": {
        "id": "c6260435"
      },
      "source": [
        "# Training & Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "064d9d76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "064d9d76",
        "outputId": "0649353f-4e2a-4bba-e8f0-5b881f005cbb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.234465873241424"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, eval_iters=200):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    for i in range(eval_iters):\n",
        "        x, y = get_batch('val')\n",
        "        logits, loss = model(x, y)\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / eval_iters\n",
        "\n",
        "estimate_loss(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "6d575624",
      "metadata": {
        "id": "6d575624"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "817fff9c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "817fff9c",
        "outputId": "b481679f-057e-48b9-b9d2-da4f55474191"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 2.4385693073272705\n",
            "Validation loss: 2.3982239317893983\n",
            "5000 2.443258762359619\n"
          ]
        }
      ],
      "source": [
        "batch_size = 32\n",
        "for steps in range(100000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 5000 == 0:\n",
        "        print(steps, loss.item())\n",
        "        if steps % 20000 == 0:\n",
        "            print(\"Validation loss:\", estimate_loss(model))\n",
        "\n",
        "    # comment out after confirmed it works\n",
        "    if steps >= 5000:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "18868156",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18868156",
        "outputId": "9493a379-6754-4cc4-991b-3f5017cbf0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "\n",
            "Ro meYo Bi\n",
            "I I t??\n",
            "Rate..\n",
            "\n",
            "Oknt?\n",
            "Ye Ge Songh e dlea s s fr ixtribu ond No Gury Ify.\n",
            "Oks t biar. oe\n"
          ]
        }
      ],
      "source": [
        "# Generation\n",
        "print(decode(\n",
        "    model.generate(x=torch.zeros((1, 1), dtype=torch.long).to(device), n_tokens=100)[0]\n",
        "))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "842bb0b2",
      "metadata": {
        "id": "842bb0b2"
      },
      "source": [
        "## Self attention\n",
        "\n",
        "Self attention is a communication mechanism."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "2bf4eab0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2bf4eab0",
        "outputId": "d8c12d81-b14f-412a-c5fd-3dfd64d8603e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "values = \n",
            " tensor([[5., 8.],\n",
            "        [0., 5.],\n",
            "        [3., 4.]])\n",
            "----------\n",
            "first_k_avg = \n",
            " tensor([[5.0000, 8.0000],\n",
            "        [2.5000, 6.5000],\n",
            "        [2.6667, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(415)\n",
        "values = torch.randint(0, 10, (3, 2)).float()\n",
        "\n",
        "# Version 1: naive for-loop\n",
        "first_k_avg = torch.empty_like(values)\n",
        "for i in range(first_k_avg.shape[0]):\n",
        "    for j in range(first_k_avg.shape[1]):\n",
        "        first_k_avg[i, j] = values[:i + 1, j].mean()\n",
        "\n",
        "print(\"values = \\n\", values)\n",
        "print('-' * 10)\n",
        "print(\"first_k_avg = \\n\", first_k_avg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "6de91d83",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6de91d83",
        "outputId": "7f412788-86fb-4d8c-9589-f82e05d08093"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei = \n",
            " tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "----------\n",
            "first_k_avg = \n",
            " tensor([[5.0000, 8.0000],\n",
            "        [2.5000, 6.5000],\n",
            "        [2.6667, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "# Version 2: mat mul\n",
        "num_rows = values.shape[0]\n",
        "wei = torch.tril(torch.ones(num_rows, num_rows))\n",
        "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
        "print(\"wei = \\n\", wei)\n",
        "print('-' * 10)\n",
        "print(\"first_k_avg = \\n\", wei @ values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "39ba4a3c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ba4a3c",
        "outputId": "88c6e630-b27b-40ea-bc46-9d6f90f4a2b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei before softmax:\n",
            " tensor([[0., -inf, -inf],\n",
            "        [0., 0., -inf],\n",
            "        [0., 0., 0.]])\n",
            "----------\n",
            "wei after softmax:\n",
            " tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "----------\n",
            "first_k_avg = \n",
            " tensor([[5.0000, 8.0000],\n",
            "        [2.5000, 6.5000],\n",
            "        [2.6667, 5.6667]])\n"
          ]
        }
      ],
      "source": [
        "# Version 3: softmax\n",
        "wei = torch.zeros((num_rows, num_rows))\n",
        "tril = torch.tril(torch.ones_like(wei))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "print(\"wei before softmax:\\n\", wei)\n",
        "print('-' * 10)\n",
        "wei = F.softmax(wei, dim=1)\n",
        "print(\"wei after softmax:\\n\", wei)\n",
        "print('-' * 10)\n",
        "print(\"first_k_avg = \\n\", wei @ values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "df8e9b50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "df8e9b50",
        "outputId": "0f9b260a-cc3a-4e16-adde-95b67cdf12e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wei:\n",
            " tensor([[[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4607, 0.5393, 0.0000, 0.0000],\n",
            "         [0.3991, 0.2089, 0.3920, 0.0000],\n",
            "         [0.2587, 0.2108, 0.2907, 0.2397]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4038, 0.5962, 0.0000, 0.0000],\n",
            "         [0.2823, 0.3892, 0.3285, 0.0000],\n",
            "         [0.2289, 0.2493, 0.2322, 0.2896]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3789, 0.6211, 0.0000, 0.0000],\n",
            "         [0.3268, 0.3199, 0.3533, 0.0000],\n",
            "         [0.1970, 0.2184, 0.2262, 0.3584]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4430, 0.5570, 0.0000, 0.0000],\n",
            "         [0.2956, 0.3503, 0.3541, 0.0000],\n",
            "         [0.2012, 0.2451, 0.2963, 0.2574]]], grad_fn=<SoftmaxBackward0>)\n",
            "----------\n",
            "weight vector for third prediction:\n",
            " tensor([0.3991, 0.2089, 0.3920, 0.0000], grad_fn=<SelectBackward0>)\n",
            "out shape: torch.Size([4, 4, 16])\n"
          ]
        }
      ],
      "source": [
        "# Version 4: serlf attention\n",
        "\n",
        "B, T, C = 4, 4, 32  # batch, time, channels (embeddings)\n",
        "x = torch.randn(B, T, C)\n",
        "\n",
        "# Single head self attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "\n",
        "k = key(x)  # (B, T, 16)\n",
        "q = query(x)  # (B, T, 16)\n",
        "wei = q @ k.transpose(-2, -1) * C**-0.5  # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "\n",
        "print(\"wei:\\n\", wei)\n",
        "print('-' * 10)\n",
        "print(\"weight vector for third prediction:\\n\", wei[0, 2])\n",
        "print(\"out shape:\", out.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "992aa172",
      "metadata": {
        "id": "992aa172"
      },
      "source": [
        "## Notes on Key, Query, Value in self attention\n",
        "\n",
        "Attention is a communication mechanism. It works as a directed graph, passing information along some direction. In text generation, it often involves passing information from past tokens to future tokens.\n",
        "\n",
        "There are three components in self attention mechanism.\n",
        "* Query - Q(x) projects what information x is seeking\n",
        "* Key - K(x) projects what information x contains\n",
        "* Value - V(x) determines what information should be aggregated for the purpose of this single attention head\n",
        "\n",
        "To understand Q, K, V intuitively,\n",
        "* Think of x like private information or private key of a token, it is then projected into the Query, Key, and Value handled by the attention head.\n",
        "* The output of self attention is a weighted sum of the projection V(x), not the tokens themselves. Why?\n",
        "    * It enables us to simultaneously consider various aspects of tokens in different heads after we introduce multi-head attention mechanism next.\n",
        "    * For example, in processing the word \"cat\" within a sentence, different attention heads might aggregate information with regards to its grammatical role (noun), its conceptual meaning as an animal, or its syntactic function as a subject or object. This diversity allows for a richer, more nuanced understanding of text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "3a46805e",
      "metadata": {
        "id": "3a46805e"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "    '''Single head self attention'''\n",
        "\n",
        "    def __init__(self, d_emb, d_head):\n",
        "        super().__init__()\n",
        "        self.d_head = d_head\n",
        "        self.key = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.query = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.value = nn.Linear(d_emb, d_head, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, T, d_emb\n",
        "        B, T, C = x.shape\n",
        "        k = self.key(x)  # B, T, dh\n",
        "        q = self.query(x)  # B, T, dh\n",
        "        v = self.value(x)  # B, T, dh\n",
        "        wei = q @ k.transpose(-2, -1) * (self.d_head ** -0.5)  # B, T, T\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
        "        wei = F.softmax(wei, dim=-1)\n",
        "        out = wei @ v  # B, T, dh\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHead(nn.Module):\n",
        "    '''Multi head self attention'''\n",
        "\n",
        "    def __init__(self, num_heads, d_emb, d_head):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(d_emb, d_head) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(num_heads * d_head, d_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, T, d_emb\n",
        "        head_out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.proj(head_out)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55af815e",
      "metadata": {
        "id": "55af815e"
      },
      "source": [
        "## Final Notes on Attention Mechanism\n",
        "\n",
        "1. **Position Encoding**: Attention mechanisms inherently lack a notion of order, unlike convolutions. Therefore, inputs to the attention mechanism should include positional information to maintain sequence context.\n",
        "2. **Scaled Attention**: To prevent the softmax function from collapsing into a one-hot vector, it's crucial that the weights (Q @ K) are diffused appropriately, hence the need for scaling attention (divided by d_head**0.5).\n",
        "3. **Batch Isolation**: Examples within the same batch do not interact; each instance is processed independently.\n",
        "4. **Transformer Architecture Variations**:\n",
        "    - **Decoder Block**: Restricts information flow to prevent future tokens from influencing the output, typically used in output generation phases.\n",
        "    - **Encoder Block**: Allows free communication among all nodes, fully utilizing context, typically used in input interpretation phases.\n",
        "    - **Application in the Transformer paper**: In context of machine translation, the original paper uses Encoder Blocks for source language text, encoding full contextual understanding, and uses Decoder Blocks for target language text, ensuring generated content is influenced only by preceding text and the source content.\n",
        "\n",
        "In addition to self-attention, a transformer block comprises:\n",
        "\n",
        "- **Computation Layer**: a feedforward network computes over the aggregated information, but on per token basis (no communication between two tokens at this step)\n",
        "- **Optimization Techniques for Deep Networks**:\n",
        "    - **Residual Connection**: Facilitates learning by creating shortcuts for gradients, acting as a \"super-highway\" for backpropagation.\n",
        "    - **Layer Normalization**: Standardizes the inputs to each layer, ensuring consistent scale and aiding in stable training.\n",
        "    - **Dropout**: Randomly omits a subset of features at each layer to prevent overfitting and encourage generalized representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f02b6dc",
      "metadata": {
        "id": "7f02b6dc"
      },
      "source": [
        "## Attention Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "1e7183f2",
      "metadata": {
        "id": "1e7183f2"
      },
      "outputs": [],
      "source": [
        "class LayerNorm1d(nn.Module):\n",
        "    '''Layer normalization over the last dimension'''\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.gamma = torch.ones(dim, device=device)\n",
        "        self.beta = torch.zeros(dim, device=device)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # In batch norm, we aggregate over columns\n",
        "        # In layer norm, we aggregate over rows\n",
        "        xmean = x.mean(-1, keepdim=True)\n",
        "        xvar = x.var(-1, keepdim=True)\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)\n",
        "        out = self.gamma * xhat + self.beta\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "id": "c534accc",
      "metadata": {
        "id": "c534accc"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    '''Transformer block with multi-head attention and feedforward'''\n",
        "\n",
        "    def __init__(self, n_emb, n_head):\n",
        "        super().__init__()\n",
        "        d_head = n_emb // n_head\n",
        "        self.attn = MultiHead(n_head, n_emb, d_head)\n",
        "        self.ffwd = FeedForward(n_emb, n_emb, dropout=0.1)\n",
        "        self.ln1 = LayerNorm1d(n_emb)\n",
        "        self.ln2 = LayerNorm1d(n_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: B, T, n_emb\n",
        "        # with residual connection\n",
        "        x = x + self.attn(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "id": "be931d4b",
      "metadata": {
        "id": "be931d4b"
      },
      "outputs": [],
      "source": [
        "# v0.3 - with the transformer block\n",
        "class SimpleLanguageModelWithTransformer(nn.Module):\n",
        "    '''Simple Bigram language model with transformer layers'''\n",
        "    def __init__(self, d_vocab, d_emb, num_heads, n_layers):\n",
        "        super().__init__()\n",
        "        self.token_emb = nn.Embedding(d_vocab, d_emb)  # B, T -> B, T, d_emb\n",
        "        self.position_emb = nn.Embedding(block_size, d_emb)  # B, T -> B, T, d_emb\n",
        "        # Note: Changed here\n",
        "        # self.ffwd = FeedForward(d_emb, d_ff, dropout)  # B, T, d_emb -> B, T, d_emb\n",
        "        self.blocks = nn.Sequential(\n",
        "            *[TransformerBlock(d_emb, num_heads) for _ in range(n_layers)]\n",
        "        )\n",
        "        self.ln_final = LayerNorm1d(d_emb)\n",
        "        self.lm_head = nn.Linear(d_emb, d_vocab)  # B, T, d_emb -> B, T, d_vocab\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        _, T = x.shape\n",
        "        token_emb = self.token_emb(x) # B, T, d_emb\n",
        "        pos_emb = self.position_emb(torch.arange(T, device=device))\n",
        "        x = token_emb + pos_emb\n",
        "        x = self.blocks(x)\n",
        "        x = self.ln_final(x)\n",
        "        logits = self.lm_head(x) # B, T, d_vocab\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, d_vocab = logits.shape\n",
        "            logits = logits.view(-1, d_vocab)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, x, n_tokens):\n",
        "        # B, T = x.shape\n",
        "        for _ in range(n_tokens):\n",
        "            x_cond = x[:, -block_size:]\n",
        "            logits, loss = self(x_cond)  # logits: B, T, d_vocab\n",
        "            logits = logits[:, -1, :]  # B, d_vocab\n",
        "            probs = F.softmax(logits, dim=-1)  # B, d_vocab\n",
        "            x_next = torch.multinomial(probs, num_samples=1)  # B, 1\n",
        "            x = torch.cat((x, x_next), dim=1)  # B, T+1\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "ca14a08d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ca14a08d",
        "outputId": "5dd30e8d-39af-4509-8408-38df0d6d6652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "58949\n"
          ]
        }
      ],
      "source": [
        "n_layer = 2\n",
        "num_heads = 4\n",
        "model = SimpleLanguageModelWithTransformer(d_vocab, d_emb, num_heads, n_layer)\n",
        "model = model.to(device)\n",
        "print(sum(p.numel() for p in model.parameters() if p.requires_grad))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DEBUG = False"
      ],
      "metadata": {
        "id": "n3uoNZqVvWSq"
      },
      "id": "n3uoNZqVvWSq",
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "ccea345f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ccea345f",
        "outputId": "3a2a84b7-cb22-48eb-eb3f-5f72d691171d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 3.789302349090576\n",
            "Validation loss: 3.76059205532074\n",
            "5000 1.8052563667297363\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(100000):\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if steps % 5000 == 0:\n",
        "        print(steps, loss.item())\n",
        "        if steps % 20000 == 0:\n",
        "            print(\"Validation loss:\", estimate_loss(model))\n",
        "\n",
        "    # comment out after confirmed it works\n",
        "    if steps >= 5000:\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "id": "bbfb2da0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbfb2da0",
        "outputId": "83d74ff3-11d0-409e-a22b-4438fd2f8694"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss: 1.6444687110185623\n"
          ]
        }
      ],
      "source": [
        "print(\"Validation loss:\", estimate_loss(model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generation\n",
        "DEBUG = False\n",
        "print(decode(\n",
        "    model.generate(x=torch.zeros((1, 1), dtype=torch.long, device=device), n_tokens=500)[0][8:]\n",
        "))"
      ],
      "metadata": {
        "id": "3UmBDz96qi5C",
        "outputId": "63208cfe-892a-45a6-bfdd-226b38177cdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "3UmBDz96qi5C",
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eller\n",
            "Soeme!\n",
            "\n",
            "Chandler Bing\n",
            "When you hing.\n",
            "\n",
            "Chandler Bing\n",
            "Hinmonot misle coofel come, can?\n",
            "\n",
            "Chandler Bing\n",
            "Oh, kid. Will!! I want a peieeve a contion..\n",
            "\n",
            "Phoebe Buffay\n",
            "Thank tod Geller\n",
            "I dreally to of\n",
            "Phoebe Buffay\n",
            "Oh, God!\n",
            "\n",
            "Rachel Greeen\n",
            "The boweent?\n",
            "\n",
            "Joey Tribbiani.\n",
            "\n",
            "Enight, hours sorked a does marring to abounne\n",
            "Low, you. Okay, love make off too him there peally the doesso Tribbiani\n",
            "Thank robages?!\n",
            "\n",
            "Chandler Bing\n",
            "Yeah!\n",
            "\n",
            "Joey Tribbiani\n",
            "Whahis wait, you frob Joey givery need heave hur I ju\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKjQhPvktlTO"
      },
      "id": "uKjQhPvktlTO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}